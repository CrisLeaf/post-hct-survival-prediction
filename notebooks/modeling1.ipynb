{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('muted')\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/equity-post-HCT-survival-predictions/'\n",
    "RANDOM_STATE = 54321"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(DATA_PATH + 'sample_submission.csv')\n",
    "test_df = pd.read_csv(DATA_PATH + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_pickle(DATA_PATH + 'train_test_split/X_21-12-2024.pkl')\n",
    "y = pd.read_pickle(DATA_PATH + 'train_test_split/y_21-12-2024.pkl')\n",
    "efs_time = pd.read_pickle(DATA_PATH + 'train_test_split/efs_time_21-12-2024.pkl')\n",
    "race_group = pd.read_pickle(DATA_PATH + 'train_test_split/race_group_21-12-2024.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "import numpy as np\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    >>> import pandas as pd\n",
    "    >>> row_id_column_name = \"id\"\n",
    "    >>> y_pred = {'prediction': {0: 1.0, 1: 0.0, 2: 1.0}}\n",
    "    >>> y_pred = pd.DataFrame(y_pred)\n",
    "    >>> y_pred.insert(0, row_id_column_name, range(len(y_pred)))\n",
    "    >>> y_true = { 'efs': {0: 1.0, 1: 0.0, 2: 0.0}, 'efs_time': {0: 25.1234,1: 250.1234,2: 2500.1234}, 'race_group': {0: 'race_group_1', 1: 'race_group_1', 2: 'race_group_1'}}\n",
    "    >>> y_true = pd.DataFrame(y_true)\n",
    "    >>> y_true.insert(0, row_id_column_name, range(len(y_true)))\n",
    "    >>> score(y_true.copy(), y_pred.copy(), row_id_column_name)\n",
    "    0.75\n",
    "    \"\"\"\n",
    "    \n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "    \n",
    "    event_label = 'efs'\n",
    "    interval_label = 'efs_time'\n",
    "    prediction_label = 'prediction'\n",
    "    for col in submission.columns:\n",
    "        if not pandas.api.types.is_numeric_dtype(submission[col]):\n",
    "            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n",
    "    # Merging solution and submission dfs on ID\n",
    "    merged_df = pd.concat([solution, submission], axis=1)\n",
    "    merged_df.reset_index(inplace=True)\n",
    "    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n",
    "    metric_list = []\n",
    "    for race in merged_df_race_dict.keys():\n",
    "        # Retrieving values from y_test based on index\n",
    "        indices = sorted(merged_df_race_dict[race])\n",
    "        merged_df_race = merged_df.iloc[indices]\n",
    "        # Calculate the concordance index\n",
    "        c_index_race = concordance_index(\n",
    "                        merged_df_race[interval_label],\n",
    "                        -merged_df_race[prediction_label],\n",
    "                        merged_df_race[event_label])\n",
    "        metric_list.append(c_index_race)\n",
    "    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_wrapper(y_true, y_pred, efs_time, race_group):\n",
    "    y_pred_dict = {\n",
    "        'prediction': {i: pred for i, pred in enumerate(y_pred)}\n",
    "    }        \n",
    "    y_pred_dict = pd.DataFrame(y_pred_dict)\n",
    "    y_pred_dict.insert(0, 'id', range(len(y_pred_dict)))\n",
    "    \n",
    "    y_true_dict = {\n",
    "        'efs': {i: y for i, y in enumerate(y_true.values)},\n",
    "        'efs_time': {i: t for i, t in enumerate(efs_time.values)},\n",
    "        'race_group': {i: r for i, r in enumerate(race_group.values)},\n",
    "    }\n",
    "    y_true_dict = pd.DataFrame(y_true_dict)\n",
    "    y_true_dict.insert(0, 'id', range(len(y_true_dict)))\n",
    "    \n",
    "    return score(y_true_dict.copy(), y_pred_dict.copy(), 'id')\n",
    "\n",
    "def cross_validate(model, X, y, cv=10):\n",
    "    cv_scores = []\n",
    "    \n",
    "    for i in tqdm(range(cv)):\n",
    "        test_idxs = list(range(int((len(X)*(i)/cv)), int((len(X)*(i+1)/cv))))\n",
    "        \n",
    "        X_train = X.drop(index=test_idxs)\n",
    "        y_train = y.drop(index=test_idxs)\n",
    "        X_test = X.iloc[test_idxs]\n",
    "        y_test = y.iloc[test_idxs]\n",
    "        \n",
    "        model_copy = clone(model)\n",
    "        \n",
    "        model_copy.fit(X_train, y_train)\n",
    "        y_pred = model_copy.predict(X_test)\n",
    "        \n",
    "        cv_scores.append(score_wrapper(\n",
    "            y_test, \n",
    "            y_pred, \n",
    "            efs_time.iloc[test_idxs], \n",
    "            race_group.iloc[test_idxs]\n",
    "        ))\n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score: 0.5901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = LGBMRegressor(verbose=-1)\n",
    "\n",
    "cv_score = cross_validate(model, X, y, cv=10)\n",
    "\n",
    "print(f'CV Score: {cv_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best cv: 0.5901\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv3-10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
